{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Matching\n",
    "    \n",
    "    \n",
    "Let's consider that you, as a data scientist, are working for **Retailer A**, a large-scale software products chain. Retailer A has recently entered into a strategic partnership with **Retailer B**, an online software product platform. As part of this partnership, Retailer B has shared its product dataset with Retailer A for the purpose of cross-promotion, product indexing and targeted marketing.\n",
    "\n",
    "Your task is to perform **entity resolution**, also known as record linkage, on these datasets. The goal is to identify which products in Retailer B's dataset are also products of Retailer A. This will allow the marketing department to create more personalized product offering campaigns and product indexes.\n",
    "\n",
    "Entity resolution can be a complex task due to various reasons such as data inconsistencies, missing values, and the need to protect customer privacy. It involves several steps including data cleaning, data standardization, and matching records.\n",
    "\n",
    "You may find the datasets in CSV format:\n",
    "\n",
    "    - DDB_retailerA.csv\n",
    "    - DDB_retailerB.csv\n",
    "    \n",
    "Notes:\n",
    "\n",
    "- Schema aligned\n",
    "- Structured data (no large mess ups, void cells, etc ...)\n",
    "- Blocking already performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity resolution\n",
    "\n",
    "Data stored in information systems are often erroneous. The\n",
    "most typical errors include: inconsistent, missing, and outdated\n",
    "values, typos as well as duplicates. To handle data of poor quality,\n",
    "data cleaning (a.k.a. curation) and deduplication (a.k.a. entity\n",
    "resolution) methods are used in projects realized by research\n",
    "and industry. This is of particular challenge due\n",
    "to its computational complexity and the complexity of finding\n",
    "the most adequate method for comparing records and computing\n",
    "similarities of these records. The similarity value of two records\n",
    "is a compound value, whose computation is based on similarities\n",
    "of individual attribute values.\n",
    "\n",
    "\n",
    "Recall that an ER pipeline includes four basic tasks, namely:\n",
    "    \n",
    "- Blocking (a.k.a. indexing) - it organizes records into groups,\n",
    "such that each group includes records that may include\n",
    "potential duplicates.\n",
    "\n",
    "- Block processing (a.k.a. filtering) - its goal is to eliminate\n",
    "records that do not have to be compared.\n",
    "\n",
    "- **Entity matching** (a.k.a. similarity computation) - it computes similarity values between records compared in pairs,\n",
    "i.e., a value of each attribute in one record is compared to\n",
    "a value of a corresponding attribute in the second record.\n",
    "\n",
    "- **Entity clustering** - it aims at creating groups of similar records, from pairs of records representing highly probable duplicates.\n",
    "\n",
    "\n",
    "For simplicity, we are going to focus on entity matching and entity clustering. This is a fairly basic pipeline and it can be extended in many ways. For example, sophisticated pre-processing and matching algorithms can be used. Or thinking about how to scale the entity resolution (i.e. speeding up the process without sacrificying accuracy).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverable format and tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the assignment,we will use a python script (*ddb_entity_matching.py*) that takes a data folder containing 2 CSV datasets and outputs a single CSV file called \"match_report.csv\" with the format below. You may use jupyter notebook to play with you algortihmic solution or other entity matching tools. However, the final deliverable will contain the aforementioned script, this jupyter notebook and requirements.txt file with all the libraries.\n",
    "\n",
    "It should work this way: python ddb_entity_matching.py -i retailer_data_folder/ -o match_report_file_path\n",
    "\n",
    "- OUTPUT SCHEMA:\n",
    "\n",
    "| retailerA_id | retailerB_id | match |   |   |\n",
    "|--------------|--------------|-------|---|---|\n",
    "| 571          | 946          | 0     |   |   |\n",
    "| 574          | 2423         | 0     |   |   |\n",
    "| 250          | 2839         | 0     |   |   |\n",
    "| 1162         | 2109         | 1     |   |   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data cleaning\n",
    "\n",
    "Perform data cleaning with the help of regex expressions, removal of stopwords and the tokenization of the documents/sentences or paragraphs. Things like casing, extra spaces, quotes and new lines can be ignored (not limited to that though).\n",
    "\n",
    "spaCy provides a one-stop-shop for tasks commonly used in any NLP project, so they might have all these functionalities. It useful to explore other packages like NTLK, or build the cleaning steps yourself.\n",
    "\n",
    "\n",
    "The recommended workflow is the following: Read in the CSV files and apply the data cleaning operations.\n",
    "\n",
    "***Hint***: https://www.nltk.org/api/nltk.tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kaana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kaana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = str(text).lower()\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Tokenize and remove stopwords\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "def load_and_clean_data(file_path, name_column):\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Clean the product name column\n",
    "    df['product_name_clean'] = df[name_column].apply(clean_text)\n",
    "    return df\n",
    "\n",
    "# Load and clean datasets\n",
    "df_a = load_and_clean_data('DDB_retailerA.csv', 'name')\n",
    "df_b = load_and_clean_data('DDB_retailerB.csv', 'title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Entity Matching\n",
    "\n",
    "For every product record in the retailer A, we need to compute its similarity to every record in retailer B- Think about the total number of comparisons you need to perform.\n",
    "\n",
    "A very simple approach would be to use rule matching based on matches from regular expressions.Different measures can be used to process different attribute blocks, like using string a similarity metric like Levenshtein distance for names or Jaccard similarity to compare associated groups (say lists of friends on a social network, or a list of investments).\n",
    "\n",
    "Very powerful techniques can also arise from fuzzy matching and string distance algorithms. One could also try, for example, computing text similarity among sentences/documents/paragraphs using different model or feature extractors, and then compute similarities based on vector distances (i.e. cosine,euclidean,jaccard ) similarity. You may use the following model (not restricted to though):\n",
    "\n",
    "- **Bag of Words (BoW)** Scikit-Learn, NTLK\n",
    "- **N-grams**: Scikit-Learn, NTLK\n",
    "- **TF-IDF**: Scikit-Learn, NTLK\n",
    "- **Word embedding models**: Word2Vec ( Spacy, Gensim are packages that have this functionality)\n",
    "- **Pre-trained language models**: BERT and large language models\n",
    "\n",
    "You could even feed these to an LLM (a small OSS model?). Think of computational cost! Either way, youâ€™ll have to decide on a threshold for creating a link or not as well.\n",
    "\n",
    "\n",
    "**Extra**: There are ways to avoid this large number of comparisons among entities, they involve sophisticated methods like inserting blocking pipelines. Research this area, only after you have completed all the steps, and as an. You can opt to save the intermediary files (i.e. pairwise similarity matrix) to disk to avoid computing this step every single time. There are also additional open source libraries in python that you may use for the end-to-end entity resolution or parts of it, exclusively dedicated  to ER: *RecordLinkage*, *dedupe*, *Zingg*.\n",
    "\n",
    "The scale of the dataset is also not expected to demand this level of complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Create TF-IDF vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_a = vectorizer.fit_transform(df_a['product_name_clean'])\n",
    "tfidf_b = vectorizer.transform(df_b['product_name_clean'])\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_a, tfidf_b)\n",
    "\n",
    "# Apply threshold for matches (e.g., 0.8)\n",
    "threshold = 0.8\n",
    "matches = np.where(cosine_sim >= threshold, 1, 0)\n",
    "\n",
    "# Generate match report\n",
    "match_report = []\n",
    "for i in range(matches.shape[0]):\n",
    "    for j in range(matches.shape[1]):\n",
    "        if matches[i][j] == 1:\n",
    "            match_report.append({\n",
    "                'retailerA_id': df_a.iloc[i]['id'],\n",
    "                'retailerB_id': df_b.iloc[j]['id'],\n",
    "                'match': 1\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Evaluation\n",
    "\n",
    "\n",
    "The file \"matching_labels.csv\" contains the matched pairs, a value of 1/0 represents a true/false matched pair. You may use this to gauge the performance of your solution by identyfying hits and misses, and precision and recall. These only represent about 3/5 of the total pairs, so be aware of overfitting your solution as the rest will be used to grade your solution. Precision and recall will be used to evaluate your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.61, Recall: 0.09\n"
     ]
    }
   ],
   "source": [
    "# Load ground truth labels\n",
    "true_labels = pd.read_csv('matching_labels.csv')\n",
    "\n",
    "# Convert predictions to DataFrame\n",
    "predicted_labels = pd.DataFrame(match_report)\n",
    "\n",
    "# Calculate precision and recall\n",
    "true_positives = len(pd.merge(true_labels, predicted_labels, on=['retailerA_id', 'retailerB_id']))\n",
    "false_positives = len(predicted_labels) - true_positives\n",
    "false_negatives = len(true_labels) - true_positives\n",
    "\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "print(f'Precision: {precision:.2f}, Recall: {recall:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Entity clustering\n",
    "\n",
    "Once entities have been resolved as matches. we need to  merge them in order to produce a single representative record. This involves grouping the entities based on the similarity scores. Entities or records in the same cluster are considered to be the same.\n",
    "\n",
    "*Hint: You can use different clustering algorithms, but graph clustering algorithms are the most suitable approach. Think that the output of the entity mathcing stage can be a similarity graph or matrix, you may use tools such as networkX package to apply the needed transformations*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a clustering procedure that identifies and groups all similar records \n",
    "import networkx as nx\n",
    "\n",
    "# Create similarity graph\n",
    "G = nx.Graph()\n",
    "for pair in match_report:\n",
    "    G.add_edge(pair['retailerA_id'], pair['retailerB_id'])\n",
    "\n",
    "# Find connected components (clusters)\n",
    "clusters = list(nx.connected_components(G))\n",
    "\n",
    "# Save clusters to DataFrame (example)\n",
    "cluster_report = []\n",
    "for cluster_id, cluster in enumerate(clusters):\n",
    "    for node in cluster:\n",
    "        cluster_report.append({\n",
    "            'entity_id': node,\n",
    "            'cluster_id': cluster_id\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Generate Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(match_report).to_csv('match_report.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
